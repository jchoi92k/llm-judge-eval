{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb247762",
   "metadata": {},
   "source": [
    "# THIS IS AN EXAMPLE NOTEBOOK\n",
    "This is meant to serve as a placeholder for reference. Please use the `run_evaluation_{team_name}.ipynb` notebook provided in your team-specific package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d42aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================EXAMPLE NOTEBOOK=================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e541003",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline - Example\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Virtual Environment\n",
    "Use `uv` or regular `pip` to install dependencies. Below is an example snippet on how to set up a virtual environment\n",
    "```bash\n",
    "uv python install 3.12.10\n",
    "uv venv .venv --python 3.12.10\n",
    ".venv\\Scripts\\activate.bat # on Windows; or source .venv/bin/activate on Mac\n",
    "uv pip install -r requirements.txt\n",
    "uv pip install ipykernel\n",
    "python -m ipykernel install --user --name=venv --display-name \"Evaluator\"\n",
    "```\n",
    "\n",
    "**Note**: After installing the kernel, you may need to:\n",
    "- Restart your Jupyter server or refresh the browser\n",
    "- Reload VS Code window (Ctrl+Shift+P → \"Reload Window\")\n",
    "- Select the \"Evaluator\" kernel from the kernel picker\n",
    "\n",
    "### OpenAI API Key\n",
    "Add your `OPENAI_API_KEY` to an `.env` file in the project root.\n",
    "\n",
    "### Required Data Files\n",
    "Place the following files in `inputs\\session_data\\`:\n",
    "1. `session_data.csv` - Your session data to evaluate\n",
    "2. `human_evaluation.csv` - Human evaluation data (optional, but recommended)\n",
    "\n",
    "File paths and names are configurable in [`config.toml`](config.toml).\n",
    "\n",
    "**Data Preprocessing:** Use the helper function in `evaluation_pipeline.utils` or the full notebook at `data_preprocessing.ipynb` to convert data packages into `session_data.csv`.\n",
    "\n",
    "### Configuration\n",
    "All evaluation settings are managed through `config.toml`. See [`CONFIG.md`](CONFIG.md) for detailed options.\n",
    "The current notebook is configured to sample `5` rows out of the full session data for evaluation.\n",
    "\n",
    "## More Information\n",
    "For more information, see [`README.md`](README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc035dea",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "**Important** - Before running these cells, refer to `data_preprocessing.ipynb` to format your data package file into `session_data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb491076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import sys\n",
    "from evaluation_pipeline import Config, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_toml(\"placeholder_config.toml\") # Specify your config file here; this will not run as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(config.dirs.logs / f'{config.run_id}.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Suppress noisy loggers\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac03df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluator instance\n",
    "evaluator = Evaluator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caccd866",
   "metadata": {},
   "source": [
    "## 2. Full Run\n",
    "\n",
    "**Note:** For your first time, we recommend stepping through the remaining cells to understand each stage of the evaluation process before running the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e483b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run the full pipeline\n",
    "# evaluator.run(\n",
    "#     auto_approve=True,  # Skip cost estimate approval prompts\n",
    "#     mode=\"batch\",  # 'batch' or 'flex' (same pricing)\n",
    "#     skip_adjudication=False, \n",
    "#     check_interval=60, # Check every 60 seconds for completed evaluations\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2342807",
   "metadata": {},
   "source": [
    "## 3. Step-by-Step Run\n",
    "\n",
    "### Overview\n",
    "The overall flow of the evaluation is as follows:\n",
    "\n",
    "1. Session data and human evaluation data are loaded. The config file and session data file contents are hashed to create a `run_id`. All artifacts will have this `run_id` as an affix.\n",
    "2. Generate evaluation guidelines using human evaluation data. **[Section A]**\n",
    "3. Generate context augmented dynamic prompts for all session data. **[Section B1/B2]**\n",
    "4. Run evaluations twice. **[Section B1/B2]**\n",
    "5. Generate dynamic prompts for evaluations that require adjudication (= ANY of the subcriteria have a score gap >= 2, or if there is disagreement on whether the input are mathematically relevant). **[Section B1/B2]**\n",
    "6. Run adjudication. **[Section B1/B2]**\n",
    "7. Generate final scores. **[Section C]**\n",
    "\n",
    "**Note:** Choose either **Section B1** (Flex Processing) or **Section B2** (Batch Processing) for evaluation—not both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73996afc",
   "metadata": {},
   "source": [
    "### A. Generate Evaluation Guidelines\n",
    "Evaluation guidelines are generated using the following components as contextual information:\n",
    "- Human evaluations\n",
    "- Math intervention practice guides from Doing What Works\n",
    "- The evaluation rubric\n",
    "- Data description, tool description, and tool-specific considerations\n",
    "\n",
    "The guideline generation runs three times: the first two runs generate guidelines independently, and the third run aggregates them to create a more stable, consistent evaluation guideline.\n",
    "\n",
    "**Recommended:** Review and refine the generated guideline manually before proceeding to ensure it aligns with your evaluation goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3741a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.generate_evaluation_guidelines(auto_approve=False, force_regenerate=False, test_run=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c2f90",
   "metadata": {},
   "source": [
    "### B1. Evaluation: Flex Processing (Direct API Calls)\n",
    "\n",
    "Run evaluations using direct API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dynamic prompts for evaluation\n",
    "evaluator.generate_dynamic_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial evaluations (2 per session)\n",
    "# Provides cost estimate and requires user input to proceed - Set auto_approve=True to skip cost approval prompts\n",
    "evaluator.flex_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run check_evaluation_status at any point to see current status and suggested next steps\n",
    "evaluator.check_evaluation_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17138847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this part unless Next steps: Adjudication is suggested\n",
    "# Generate dynamic prompts for adjudication\n",
    "evaluator.generate_dynamic_prompts(adjudication=True)\n",
    "evaluator.flex_evaluate(adjudication=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac709a",
   "metadata": {},
   "source": [
    "### B2. Evaluation: Batch Processing \n",
    "\n",
    "Run evaluations using OpenAI's batch API for cost savings (50% off).\n",
    "Follows the same evaluation flow as flex processing, but uses batch mode instead.\n",
    "\n",
    "**Note:** If you've already run flex evaluation (B1), skip this section. Choose either flex or batch processing, not both. You can delete the output files or rename the output file path in `config.toml` to start over.\n",
    "\n",
    "**Troubleshooting:**\n",
    "- **Kernel restart during batch**: Use `batch_id_override` to retrieve results:\n",
    "```python\n",
    "  # Find your batch_id in logs or OpenAI dashboard\n",
    "  evaluator.check_and_retrieve(\n",
    "      until_complete=True, \n",
    "      batch_id_override=\"batch_abc123\"\n",
    "  )\n",
    "```\n",
    "- **Check status manually**: \n",
    "```python\n",
    "  evaluator.check_batch_status() \n",
    "```\n",
    "- **Cancel batch**:\n",
    "```python\n",
    "  evaluator.cancel_batch()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cc935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dynamic prompts for evaluation\n",
    "evaluator.generate_dynamic_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da361c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch file\n",
    "evaluator.prepare_batch_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.upload_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set until_complete to False if you want to run this in a non-blocking way and check back later for results.\n",
    "evaluator.check_and_retrieve(until_complete=True, check_interval=60)\n",
    "\n",
    "# Use batch_id_override to specify a particular batch ID after kernel restart\n",
    "# evaluator.check_and_retrieve(batch_id_override=\"your_batch_id_here\", until_complete=True, check_interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjudication batch (if needed)\n",
    "evaluator.generate_dynamic_prompts(adjudication=True)\n",
    "evaluator.prepare_batch_file(adjudication=True)\n",
    "evaluator.upload_batch()\n",
    "evaluator.check_and_retrieve(until_complete=True, check_interval=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf959b",
   "metadata": {},
   "source": [
    "### C. Finalize Results\n",
    "\n",
    "Generate final scores by aggregating/adjudicating evaluations.\n",
    "\n",
    "**What happens:**\n",
    "- For sessions with 3 evaluations (2 + adjudication): Uses adjudicated score\n",
    "- For sessions with 2 evaluations (no disagreement): Averages the scores\n",
    "- For sessions with 1 evaluation: Marks it as incomplete\n",
    "- Saves to the `evaluation_results` directory path defined in `config_{tool_name}.toml`\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"session_id_1\": {\n",
    "    \"scores\": {\n",
    "      \"Mathematical_Accuracy\": {\n",
    "        \"Validity\": <1-4 or null>,\n",
    "        \"Clarity_and_Labeling\": <1-4 or null>,\n",
    "        \"Justification_and_Explanation\": <1-4 or null>\n",
    "      },\n",
    "      \"Pedagogical_Quality\": {\n",
    "        \"Problem_Solving_Strategies\": <1-4>,\n",
    "        \"Relevance\": <1-4>,\n",
    "        \"Scaffolded_Support\": <1-4>,\n",
    "        \"Clarity_of_Explanation\": <1-4>,\n",
    "        \"Feedback\": <1-4>,\n",
    "        \"Motivational_Engagement\": <1-4>\n",
    "      },\n",
    "      \"Equity_and_Fairness\": {\n",
    "        \"Language_neutrality\": <1-3>,\n",
    "        \"Feedback_tone\": <1-3>,\n",
    "        \"Cultural_relevance\": <1-3>\n",
    "      }\n",
    "    },\n",
    "    \"explanations\": {\n",
    "      \"Mathematical_Accuracy\": {\n",
    "        \"Validity\": \"Brief explanation with specific evidence\",\n",
    "        \"Clarity_and_Labeling\": \"Concise justification with examples\",\n",
    "        \"Justification_and_Explanation\": \"Brief reasoning with evidence\"\n",
    "      },\n",
    "      \"Pedagogical_Quality\": {\n",
    "        \"Problem_Solving_Strategies\": \"Brief explanation with evidence\",\n",
    "        \"Relevance\": \"Concise justification\",\n",
    "        \"Scaffolded_Support\": \"Brief reasoning with examples\",\n",
    "        \"Clarity_of_Explanation\": \"Concise explanation\",\n",
    "        \"Feedback\": \"Brief justification\",\n",
    "        \"Motivational_Engagement\": \"Assessment based on student responses when available\"\n",
    "      },\n",
    "      \"Equity_and_Fairness\": {\n",
    "        \"Language_neutrality\": \"Brief explanation\",\n",
    "        \"Feedback_tone\": \"Concise justification\",\n",
    "        \"Cultural_relevance\": \"Brief assessment\"\n",
    "      }\n",
    "    },\n",
    "    \"mathematical_accuracy_relevance\": {\n",
    "      \"applicable\": <true/false>,\n",
    "      \"explanation\": \"Specific analysis of whether AI output contains evaluable mathematical content\",\n",
    "      \"extracted_mathematical_content\": \"If applicable, any mathematical content extracted from the session data by the LLM judges.\",\n",
    "      \"catastrophic_errors\": \"Any significant mathematical errors made by the AI (for example, incorrect calculations such as 2+2=5, or misidentifying a square as a triangle).\"\n",
    "    }\n",
    "  },\n",
    "  \"session_id_2\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "Adjudications will also include the following field:\n",
    "```json\n",
    "  \"adjudication_notes\": {\n",
    "    \"key_discrepancies_resolved\": \"Brief summary of main disagreements and how they were resolved\",\n",
    "    \"evaluation_preferred\": \"If one evaluation was generally more accurate, note which (1 or 2) and why\"\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.generate_final_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbb703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the results json file and print its contents\n",
    "with open(config.dirs.evaluation_results / f\"{config.run_id}_final_scores.json\", \"r\") as f:\n",
    "    final_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results for viewing here\n",
    "print(json.dumps(final_results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Eval venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
